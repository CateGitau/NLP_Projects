{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for big files when commiting to github https://stackoverflow.com/questions/32953238/how-can-i-ignore-big-files-and-push-to-git-repo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "col_names = ['sentiment', 'id', 'date', 'query_string', 'user', 'text']\n",
    "data_path = '/home/cate/streamlit-projects/data/sentiment140/training.1600000.processed.noemoticon.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "sentiment                                               text\n371032          0  @cali_hypocrisy i know!!  but u know u gotta d...\n559896          0                              i cannot fall asleep \n26041           0  Finished 100 out of 131 questions for AP Euro....\n821112          4  TweetDeckk &amp; iTunes Whattaa COMBOO  Love Ittt\n589089          0  @jonacrombie dudee i knoww but i have to get u...\n"
    }
   ],
   "source": [
    "tweet_data = pd.read_csv(data_path, header = None, names = col_names, encoding=\"ISO-8859-1\",engine = 'python' ).sample(frac = 1) #shaffles the data\n",
    "tweet_data = tweet_data[['sentiment', 'text']] #disregard other columns\n",
    "print(tweet_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "allowed_chars = ' AaBbCcDdEeFfGgHhIiJjKkLlMmNnOoPpQqRrSsTtUuVvWwXxYyZz0123456789~`!@#$%^&*()-=_+[]{}|;:\",./<>?'\n",
    "punct = '!?,.@#'\n",
    "maxlen = 280\n",
    "\n",
    "def preprocess(text):\n",
    "    return ''.join([' ' + char + ' ' if char in punct else char for char in [char for char in re.sub(r'http\\S+', 'http', text, flags=re.MULTILINE) if char in allowed_chars]])[:maxlen]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flair requires a specific format for the data to be in; it wants three CSV’s that look like:\n",
    "\n",
    "\\__label__ < LABEL>      < TEXT>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the preprocessing function\n",
    "tweet_data['text'] = tweet_data['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put __label__ in front of each sentiment\n",
    "tweet_data['sentiment'] = '__label__' + tweet_data['sentiment'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the data\n",
    "import os\n",
    "\n",
    "# Create directory for saving data if it does not already exist\n",
    "data_dir = '/home/cate/streamlit-projects/twitter_sentiment/processed-data/'\n",
    "if not os.path.isdir(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "# Save a percentage of the data (you could also only load a fraction of the data instead)\n",
    "amount = 0.125\n",
    "\n",
    "tweet_data.iloc[0:int(len(tweet_data)*0.8*amount)].to_csv(data_dir + '/train.csv', sep='\\t', index=False, header=False)\n",
    "tweet_data.iloc[int(len(tweet_data)*0.8*amount):int(len(tweet_data)*0.9*amount)].to_csv(data_dir + '/test.csv', sep='\\t', index=False, header=False)\n",
    "tweet_data.iloc[int(len(tweet_data)*0.9*amount):int(len(tweet_data)*1.0*amount)].to_csv(data_dir + '/dev.csv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "In this particular example we will use **Flair** This is a popular machine learning library for state-of-the-art NLP\n",
    "\n",
    "\n",
    "Flair tutorials -> https://github.com/flairNLP/flair/tree/master/resources/docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data \n",
    "Loading the test, train and dev sets using Flair's NLPTaskDataFetcher class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2020-05-11 11:04:47,102 Reading data from /home/cate/streamlit-projects/twitter_sentiment/processed-data\n2020-05-11 11:04:47,103 Train: /home/cate/streamlit-projects/twitter_sentiment/processed-data/train.csv\n2020-05-11 11:04:47,104 Dev: /home/cate/streamlit-projects/twitter_sentiment/processed-data/dev.csv\n2020-05-11 11:04:47,104 Test: /home/cate/streamlit-projects/twitter_sentiment/processed-data/test.csv\n"
    }
   ],
   "source": [
    "from flair.datasets import ClassificationCorpus\n",
    "from pathlib import Path\n",
    "\n",
    "# load corpus containing training, test and dev data\n",
    "corpus = ClassificationCorpus(Path(data_dir),\n",
    "                                      test_file='test.csv',\n",
    "                                      dev_file='dev.csv',\n",
    "                                      train_file='train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2020-05-11 11:05:17,277 Computing label dictionary. Progress:\n100%|██████████| 160000/160000 [00:34<00:00, 4653.80it/s]2020-05-11 11:05:52,824 [b'0', b'4']\n\n"
    }
   ],
   "source": [
    "# a label dictionary to hold all the labels assigned to the text in the corpus\n",
    "\n",
    "label_dict = corpus.make_label_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Flair's provided GloVe embeddings\n",
    "\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentRNNEmbeddings\n",
    "\n",
    "word_embeddings = [WordEmbeddings('glove'),\n",
    "# these are extra word embeddings provided by Flair for superior results\n",
    "#                    FlairEmbeddings('news-forward'),\n",
    "#                    FlairEmbeddings('news-backward')\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the embeddings\n",
    "\n",
    "document_embeddings = DocumentRNNEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import TextClassifier\n",
    "\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "trainer = ModelTrainer(classifier, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "trainer.train('model-saves',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              anneal_factor=0.5,\n",
    "              patience=8,\n",
    "              max_epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing out the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "\n",
    "classifier = TextClassifier.load('model-saves/final-model.pt')\n",
    "\n",
    "pos_sentence = Sentence(preprocess('I love Python!'))\n",
    "neg_sentence = Sentence(preprocess('Python is the worst!'))\n",
    "\n",
    "classifier.predict(pos_sentence)\n",
    "classifier.predict(neg_sentence)\n",
    "\n",
    "print(pos_sentence.labels, neg_sentence.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37364bitbaseconda569edf27ad0243b9aa6880a18fbf0a13",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}